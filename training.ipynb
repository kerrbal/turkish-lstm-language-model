{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "337fa476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy\n",
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13817e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    }
   ],
   "source": [
    "the_device = torch.device(\"cuda\")\n",
    "print(\"device: \", the_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f81da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp = numpy.load(\"train_.npy\")\n",
    "#the_array = torch.tensor(temp, dtype = torch.int32)\n",
    "#torch.save(the_array, \"torch_train.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79343eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alper\\AppData\\Local\\Temp\\ipykernel_35216\\26094260.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_it = torch.load(\"torch_train.pt\")\n"
     ]
    }
   ],
   "source": [
    "train_it = torch.load(\"torch_train.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4107a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = sentencepiece.SentencePieceProcessor(\"omer_s.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c978a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "vocab_size: 16000\n",
      "data shape [T,B]: (5711, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alper\\Desktop\\formyself\\RNNs\\data.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(path).to(torch.long)\n",
      "C:\\Users\\Alper\\AppData\\Local\\Temp\\ipykernel_35216\\3719334052.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"ckpt_ep500.pt\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 | loss 0.3371 | ppl 1.40\n",
      "epoch 20 | loss 0.3270 | ppl 1.39\n",
      "epoch 30 | loss 0.3208 | ppl 1.38\n",
      "epoch 40 | loss 0.3112 | ppl 1.37\n",
      "epoch 50 | loss 0.3055 | ppl 1.36\n",
      "epoch 60 | loss 0.2934 | ppl 1.34\n",
      "epoch 70 | loss 0.2899 | ppl 1.34\n",
      "epoch 80 | loss 0.2765 | ppl 1.32\n",
      "epoch 90 | loss 0.2724 | ppl 1.31\n",
      "epoch 100 | loss 0.2633 | ppl 1.30\n",
      "epoch 110 | loss 0.2605 | ppl 1.30\n",
      "epoch 120 | loss 0.2545 | ppl 1.29\n",
      "epoch 130 | loss 0.2461 | ppl 1.28\n",
      "epoch 140 | loss 0.2442 | ppl 1.28\n",
      "epoch 150 | loss 0.2433 | ppl 1.28\n",
      "epoch 160 | loss 0.2412 | ppl 1.27\n",
      "epoch 170 | loss 0.2342 | ppl 1.26\n",
      "epoch 180 | loss 0.2322 | ppl 1.26\n",
      "epoch 190 | loss 0.2229 | ppl 1.25\n",
      "epoch 200 | loss 0.2176 | ppl 1.24\n",
      "epoch 210 | loss 0.2123 | ppl 1.24\n",
      "epoch 220 | loss 0.2091 | ppl 1.23\n",
      "epoch 230 | loss 0.2054 | ppl 1.23\n",
      "epoch 240 | loss 0.2013 | ppl 1.22\n",
      "epoch 250 | loss 0.1963 | ppl 1.22\n",
      "epoch 260 | loss 0.1959 | ppl 1.22\n",
      "epoch 270 | loss 0.1916 | ppl 1.21\n",
      "epoch 280 | loss 0.1859 | ppl 1.20\n",
      "epoch 290 | loss 0.1821 | ppl 1.20\n",
      "epoch 300 | loss 0.1833 | ppl 1.20\n",
      "epoch 310 | loss 0.1786 | ppl 1.20\n",
      "epoch 320 | loss 0.1733 | ppl 1.19\n",
      "epoch 330 | loss 0.1747 | ppl 1.19\n",
      "epoch 340 | loss 0.1700 | ppl 1.19\n",
      "epoch 350 | loss 0.1681 | ppl 1.18\n",
      "epoch 360 | loss 0.1645 | ppl 1.18\n",
      "epoch 370 | loss 0.1630 | ppl 1.18\n",
      "epoch 380 | loss 0.1595 | ppl 1.17\n",
      "epoch 390 | loss 0.1561 | ppl 1.17\n",
      "epoch 400 | loss 0.1535 | ppl 1.17\n",
      "epoch 410 | loss 0.1544 | ppl 1.17\n",
      "epoch 420 | loss 0.1522 | ppl 1.16\n",
      "epoch 430 | loss 0.1478 | ppl 1.16\n",
      "epoch 440 | loss 0.1481 | ppl 1.16\n",
      "epoch 450 | loss 0.1458 | ppl 1.16\n",
      "epoch 460 | loss 0.1418 | ppl 1.15\n",
      "epoch 470 | loss 0.1400 | ppl 1.15\n",
      "epoch 480 | loss 0.1404 | ppl 1.15\n",
      "epoch 490 | loss 0.1378 | ppl 1.15\n",
      "epoch 500 | loss 0.1351 | ppl 1.14\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import sentencepiece as spm\n",
    "\n",
    "from data import load_ids, batchify, get_batch\n",
    "from model import LSTMLM\n",
    "\n",
    "def detach_hidden(h):\n",
    "    if h is None:\n",
    "        return None\n",
    "    return tuple(t.detach() for t in h)\n",
    "\n",
    "def main():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"device:\", device)\n",
    "\n",
    "    sp = spm.SentencePieceProcessor(model_file=\"omer_s.model\")\n",
    "    vocab_size = sp.get_piece_size()\n",
    "    print(\"vocab_size:\", vocab_size)\n",
    "\n",
    "    #set hyperparameters\n",
    "    batch_size = 32\n",
    "    seq_len = 128\n",
    "    emb_dim = 256\n",
    "    hidden_dim = 512\n",
    "    num_layers = 2\n",
    "    dropout = 0.25\n",
    "    lr = 2e-3\n",
    "    epochs = 500\n",
    "    clip = 1.0\n",
    "\n",
    "    raw = load_ids(\"torch_train.pt\")\n",
    "    data = batchify(raw, batch_size, device)\n",
    "    \n",
    "    print(\"data shape [T,B]:\", tuple(data.shape))\n",
    "\n",
    "    model = LSTMLM(vocab_size, emb_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "    ckpt = torch.load(\"ckpt_ep500.pt\", map_location=device)\n",
    "    model = LSTMLM(\n",
    "        ckpt[\"vocab_size\"],\n",
    "        ckpt[\"emb_dim\"],\n",
    "        ckpt[\"hidden_dim\"],\n",
    "        ckpt[\"num_layers\"],\n",
    "        ckpt[\"dropout\"],\n",
    "    ).to(device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    opt = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        h = None\n",
    "        total_loss = 0.0\n",
    "        steps = 0\n",
    "\n",
    "        for i in range(0, data.size(0) - 1, seq_len):\n",
    "            x, y = get_batch(data, i, seq_len)\n",
    "            h = detach_hidden(h)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits, h = model(x, h)\n",
    "\n",
    "            loss = criterion(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            steps += 1\n",
    "\n",
    "        avg = total_loss / max(1, steps)\n",
    "        ppl = math.exp(avg) if avg < 20 else float(\"inf\")\n",
    "        if (ep % 10 == 0):\n",
    "            print(f\"epoch {ep:02d} | loss {avg:.4f} | ppl {ppl:.2f}\")\n",
    "        \n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"vocab_size\": vocab_size,\n",
    "                \"emb_dim\": emb_dim,\n",
    "                \"hidden_dim\": hidden_dim,\n",
    "                \"num_layers\": num_layers,\n",
    "                \"dropout\": dropout\n",
    "            }, f\"ckpt_ep{ep:02d}.pt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6691db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINED FOR 350 EPOCHS -> 0.51 loss, 1.67 perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7f4ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alper\\AppData\\Local\\Temp\\ipykernel_35216\\2901446395.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"LSTM_1_layers.pt\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YarÄ±n arifeydi. Ã–bÃ¼r gÃ¼nkÃ¼ bayram iÃ§in hazÄ±rlanan beyaz kurbanlar, kÃ¼Ã§Ã¼k Grijgal palangasÄ±nÄ±n etrafÄ±nda otluyorlardÄ±. MÄ±stÄ±k'Ä±n beÄŸenmeyip bÄ±raktÄ±ÄŸÄ± en miskin, en hasta, en ihtiyar hayvanlarÄ± bile alÄ±yor, bir gÃ¼n iÃ§inde genÃ§leÅŸtiriyor, kuyruÄŸunu, yelesini kesmeden, ÅŸeklini deÄŸiÅŸtiriyor, gÃ¶zlerini parlatÄ±yor, ÅŸahlandÄ±rÄ±yordu. MÄ±stÄ±k, henÃ¼z geldiÄŸi kasabanÄ±n hanÄ±ndan girerken yine bu herifi gÃ¶rdÃ¼. Yeni bir zarara uÄŸramÄ±ÅŸ gibi birdenbire canÄ± sÄ±kÄ±ldÄ±. Ama bozuntuya vermedi: â€” Merhaba Molla! dedi. â€” Merhaba?.. Åimdiye kadar hiÃ§ konuÅŸmamÄ±ÅŸlardÄ±. â€” Hayvan almaya mÄ± geldin? â€” Sana ne? . . . . . . . â€” Neye geldimse geldim. MÄ±stÄ±k, kirli zayÄ±f elini seyrek sarÄ± bÄ±yÄ±klarÄ±na kaldÄ±rdÄ±. Ã‡akÄ±r gÃ¶zleri bakacak yer bulamadÄ±. Renksiz dudaklarÄ±nÄ± kÄ±sarak gÃ¼lÃ¼msedi: â€” Ortak olalÄ±m be, dedi. â€” OlalÄ±m... Molla da gÃ¼lÃ¼msedi. DÃ¶ndÃ¼ler. HanÄ±n avlusuna doÄŸru yan yana yÃ¼rÃ¼dÃ¼ler. Kahvenin Ã¶nÃ¼ndeki eski peykeye oturdular. AyaklarÄ±nÄ±n dibinde iri, alacalÄ± bir tavuk \"gut, gut, gut\" diye civcivlerini gezdiriyordu. Pazar yarÄ±ndÄ±. MÄ±stÄ±k koynundan tÃ¼tÃ¼n kesesini Ã§Ä±kardÄ±. Molla'ya uzatÄ±rken, peykenin yanÄ±ndaki pencereden iÃ§eriye baÄŸÄ±rdÄ±: â€” Bize iki kahve getir. Molla: â€” Ben oruÃ§luyum! dedi. MÄ±stÄ±k anlamadÄ±: â€” Ramazanda mÄ±yÄ±z yahu? â€” HayÄ±r. â€” ÃœÃ§ aylarda mÄ±yÄ±z? â€” HayÄ±r. â€” Ey, bu ne orucu? â€” Ben bÃ¼tÃ¼n yÄ±l bir gÃ¼n yer, bir gÃ¼n tutarÄ±m! â€” Sahi mi? â€” Vallahi... MÄ±stÄ±k tÃ¼tÃ¼n kesesini tekrar koynuna soktu. EÄŸildi, camsÄ±z pencereden kahveciye: â€” Ä°stemez, kahveleri yapma, diye seslendi. Ä°Ã§inden, \"Bu gebeÅŸin kafasÄ±na ben bir kÃ¼lah geÃ§iririm!\" dedi. Kendisinin sofuluÄŸundan, kÃ¼Ã§Ã¼kken hafÄ±zlÄ±ÄŸa Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan, ama hastalandÄ±ÄŸÄ± iÃ§in vazgeÃ§tiÄŸinden, babasÄ±nÄ±n yirmi yedi defa hacca gittiÄŸinden bahsetti. Molla yere bakarak dinliyor, baÅŸÄ±nÄ± sallÄ±yor, inanÄ±yor, Rumelililerin saÄŸlam MÃ¼slÃ¼man olduklarÄ±nÄ± sÃ¶ylÃ¼yordu. MÄ±stÄ±k sordu: â€” Sen nerelisin? â€” Kayserili. â€” Kayseri nerede? â€” Bu tarafta... Molla, kÄ±sa parmaklÄ± tombul eliyle hanÄ±n kapÄ±sÄ±nÄ± gÃ¶steriyordu. MÄ±stÄ±k, geldiÄŸi ciheti hatÄ±rlayarak: â€” Konya tarafÄ±nda mÄ±? diye sordu. â€” HayÄ±r canÄ±m, daha yukarÄ±larda... MÄ±stÄ±k, Kayseri'nin nerede, hem de ne olduÄŸunu pek iyi biliyordu. Rumeli'de bÄ±raktÄ±ÄŸÄ± Ã§iftlikleri de anlattÄ±ktan sonra yaptÄ±ÄŸÄ± kapÄ±yÄ± kÃ¢fi gÃ¶rdÃ¼. Ä°ÅŸlere geÃ§ti. KonuÅŸtular, anlaÅŸtÄ±lar. O gÃ¼nden sonra ortaklÄ±ÄŸa karar verdiler. KÃ¢ra, zarara, sermayeye ortak oluyorlardÄ±. MÄ±stÄ±k yine iÃ§inden, \"Ben sana bir kÃ¼lah giydireyim de, gÃ¶r!\" dedi. Ertesi gÃ¼n\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sentencepiece as spm\n",
    "from model import LSTMLM\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, sp, prompt: str, max_new_tokens=300, temperature=0.9, top_k=50, device=\"cpu\"):\n",
    "    model.eval()\n",
    "\n",
    "    ids = sp.encode(prompt, out_type=int)\n",
    "    x = torch.tensor(ids, dtype=torch.long, device=device).unsqueeze(1)  # [S,1]\n",
    "\n",
    "    h = None\n",
    "    _, h = model(x, h) #read the prompt, construct the hidden state\n",
    "    cur = x[-1:] #\n",
    "\n",
    "    out_ids = ids[:]\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, h = model(cur, h)\n",
    "        logits = logits[0, 0] / max(1e-8, temperature)\n",
    "\n",
    "        if top_k and top_k > 0:\n",
    "            v, ix = torch.topk(logits, top_k)\n",
    "            probs = torch.softmax(v, dim=-1)\n",
    "            next_local = torch.multinomial(probs, 1).item()\n",
    "            next_id = ix[next_local].item()\n",
    "        else:\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, 1).item()\n",
    "\n",
    "        out_ids.append(next_id)\n",
    "        cur = torch.tensor([[next_id]], dtype=torch.long, device=device)\n",
    "\n",
    "    return sp.decode(out_ids)\n",
    "\n",
    "def main():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    sp = spm.SentencePieceProcessor(model_file=\"omer_s.model\")\n",
    "\n",
    "    ckpt = torch.load(\"LSTM_1_layers.pt\", map_location=device)\n",
    "    model = LSTMLM(\n",
    "        ckpt[\"vocab_size\"],\n",
    "        ckpt[\"emb_dim\"],\n",
    "        ckpt[\"hidden_dim\"],\n",
    "        ckpt[\"num_layers\"],\n",
    "        ckpt[\"dropout\"],\n",
    "    ).to(device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "    print(generate(\n",
    "        model, sp,\n",
    "        prompt=\"YarÄ±n arifeydi. Ã–bÃ¼r gÃ¼nkÃ¼ bayram iÃ§in hazÄ±rlanan beyaz kurbanlar, kÃ¼Ã§Ã¼k Grijgal palangasÄ±nÄ±n etrafÄ±nda otluyorlardÄ±.\",\n",
    "        max_new_tokens=500,\n",
    "        temperature=0.9,\n",
    "        top_k=50,\n",
    "        device=device\n",
    "    ))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aa9bba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alper\\AppData\\Local\\Temp\\ipykernel_35216\\730891572.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"LSTM_1_layers.pt\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YarÄ±n arifeydi. Ã–bÃ¼r gÃ¼nkÃ¼ bayram iÃ§in hazÄ±rlanan beyaz kurbanlar, kÃ¼Ã§Ã¼k Grijgal palangasÄ±nÄ±n etrafÄ±nda otluyorlardÄ±. MÄ±stÄ±k'Ä±n beÄŸenmeyip bÄ±raktÄ±ÄŸÄ± en miskin, en hasta, en ihtiyar hayvanlarÄ± bile alÄ±yor, bir gÃ¼n iÃ§inde genÃ§leÅŸtiriyor, kuyruÄŸunu, yelesini kesmeden\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sp = spm.SentencePieceProcessor(model_file=\"omer_s.model\")\n",
    "\n",
    "ckpt = torch.load(\"LSTM_1_layers.pt\", map_location=device)\n",
    "model = LSTMLM(\n",
    "    ckpt[\"vocab_size\"],\n",
    "    ckpt[\"emb_dim\"],\n",
    "    ckpt[\"hidden_dim\"],\n",
    "    ckpt[\"num_layers\"],\n",
    "    ckpt[\"dropout\"],\n",
    ").to(device)\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "print(generate(\n",
    "    model, sp,\n",
    "    prompt=\"YarÄ±n arifeydi. Ã–bÃ¼r gÃ¼nkÃ¼ bayram iÃ§in hazÄ±rlanan beyaz kurbanlar, kÃ¼Ã§Ã¼k Grijgal palangasÄ±nÄ±n etrafÄ±nda otluyorlardÄ±.\",\n",
    "    max_new_tokens=30,\n",
    "    temperature=0.9,\n",
    "    top_k=50,\n",
    "    device=device\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa41bf0",
   "metadata": {},
   "source": [
    "### Prompt: \"YarÄ±n arifeydi. Ã–bÃ¼r gÃ¼nkÃ¼ bayram iÃ§in hazÄ±rlanan beyaz kurbanlar, kÃ¼Ã§Ã¼k Grijgal palangasÄ±nÄ±n etrafÄ±nda otluyorlardÄ±.\"\n",
    "\n",
    "<div style=\"display:flex; gap:40px\">\n",
    "\n",
    "<div>\n",
    "\n",
    "### 1-Layer LSTM\n",
    "YarÄ±n arifeydi. Ã–bÃ¼r gÃ¼nkÃ¼ bayram iÃ§in hazÄ±rlanan beyaz kurbanlar, kÃ¼Ã§Ã¼k Grijgal palangasÄ±nÄ±n etrafÄ±nda otluyorlardÄ±. MÄ±stÄ±k'Ä±n beÄŸenmeyip bÄ±raktÄ±ÄŸÄ± en miskin, en hasta, en ihtiyar hayvanlarÄ± bile alÄ±yor, bir gÃ¼n iÃ§inde genÃ§leÅŸtiriyor, kuyruÄŸunu, yelesini kesmeden, ÅŸeklini deÄŸiÅŸtiriyor, gÃ¶zlerini parlatÄ±yor, ÅŸahlandÄ±rÄ±yordu. MÄ±stÄ±k, henÃ¼z geldiÄŸi kasabanÄ±n hanÄ±ndan girerken yine bu herifi gÃ¶rdÃ¼. Yeni bir zarara uÄŸramÄ±ÅŸ gibi birdenbire canÄ± sÄ±kÄ±ldÄ±. Ama bozuntuya vermedi: â€” Merhaba Molla! dedi. â€” Merhaba?.. Åimdiye kadar hiÃ§ konuÅŸmamÄ±ÅŸlardÄ±. â€” Hayvan almaya mÄ± geldin? â€” Sana ne? . . . . . . . â€” Neye geldimse geldim. MÄ±stÄ±k, kirli zayÄ±f elini seyrek sarÄ± bÄ±yÄ±klarÄ±na kaldÄ±rdÄ±. Ã‡akÄ±r gÃ¶zleri bakacak yer bulamadÄ±. Renksiz dudaklarÄ±nÄ± kÄ±sarak gÃ¼lÃ¼msedi: â€” Ortak olalÄ±m be, dedi. â€” OlalÄ±m... Molla da gÃ¼lÃ¼msedi. DÃ¶ndÃ¼ler. HanÄ±n avlusuna doÄŸru yan yana yÃ¼rÃ¼dÃ¼ler. Kahvenin Ã¶nÃ¼ndeki eski peykeye oturdular. AyaklarÄ±nÄ±n dibinde iri, alacalÄ± bir tavuk \"gut, gut, gut\" diye civcivlerini gezdiriyordu. Pazar yarÄ±ndÄ±. MÄ±stÄ±k koynundan tÃ¼tÃ¼n kesesini Ã§Ä±kardÄ±. Molla'ya uzatÄ±rken, peykenin yanÄ±ndaki pencereden iÃ§eriye baÄŸÄ±rdÄ±: â€” Bize iki kahve getir. Molla: â€” Ben oruÃ§luyum! dedi. MÄ±stÄ±k anlamadÄ±: â€” Ramazanda mÄ±yÄ±z yahu? â€” HayÄ±r. â€” ÃœÃ§ aylarda mÄ±yÄ±z? â€” HayÄ±r. â€” Ey, bu ne orucu? â€” Ben bÃ¼tÃ¼n yÄ±l bir gÃ¼n yer, bir gÃ¼n tutarÄ±m! â€” Sahi mi? â€” Vallahi... MÄ±stÄ±k tÃ¼tÃ¼n kesesini tekrar koynuna soktu. EÄŸildi, camsÄ±z pencereden kahveciye: â€” Ä°stemez, kahveleri yapma, diye seslendi. Ä°Ã§inden, \"Bu gebeÅŸin kafasÄ±na ben bir kÃ¼lah geÃ§iririm!\" dedi. Kendisinin sofuluÄŸundan, kÃ¼Ã§Ã¼kken hafÄ±zlÄ±ÄŸa Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan, ama hastalandÄ±ÄŸÄ± iÃ§in vazgeÃ§tiÄŸinden, babasÄ±nÄ±n yirmi yedi defa hacca gittiÄŸinden bahsetti. Molla yere bakarak dinliyor, baÅŸÄ±nÄ± sallÄ±yor, inanÄ±yor, Rumelililerin saÄŸlam MÃ¼slÃ¼man olduklarÄ±nÄ± sÃ¶ylÃ¼yordu. MÄ±stÄ±k sordu: â€” Sen nerelisin? â€” Kayserili. â€” Kayseri nerede? â€” Bu tarafta... Molla, kÄ±sa parmaklÄ± tombul eliyle hanÄ±n kapÄ±sÄ±nÄ± gÃ¶steriyordu. MÄ±stÄ±k, geldiÄŸi ciheti hatÄ±rlayarak: â€” Konya tarafÄ±nda mÄ±? diye sordu. â€” HayÄ±r canÄ±m, daha yukarÄ±larda... MÄ±stÄ±k, Kayseri'nin nerede, hem de ne olduÄŸunu pek iyi biliyordu. Rumeli'de bÄ±raktÄ±ÄŸÄ± Ã§iftlikleri de anlattÄ±ktan sonra yaptÄ±ÄŸÄ± kapÄ±yÄ± kÃ¢fi gÃ¶rdÃ¼. Ä°ÅŸlere geÃ§ti. KonuÅŸtular, anlaÅŸtÄ±lar. O gÃ¼nden sonra ortaklÄ±ÄŸa karar verdiler. KÃ¢ra, zarara, sermayeye ortak oluyorlardÄ±. MÄ±stÄ±k yine iÃ§inden, \"Ben sana bir kÃ¼lah giydireyim de, gÃ¶r!\" dedi. Ertesi gÃ¼n\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "\n",
    "### 2-Layer LSTM\n",
    "YarÄ±n arifeydi. Ã–bÃ¼r gÃ¼nkÃ¼ bayram iÃ§in hazÄ±rlanan beyaz kurbanlar, kÃ¼Ã§Ã¼k Grijgal palangasÄ±nÄ±n etrafÄ±nda otluyorlardÄ±. Ä°ÅŸleri ansÄ±zÄ±n giydiriyordu. ğŸ™ğŸ™Ÿ Biraz sonra... Siyah aÄŸaÃ§larÄ±n arkasÄ±ndan doÄŸan kan renginde bir ay, bizim gÃ¶zlerini bayraktÄ±yordu. Dar masanÄ±n orta yerinde duruyordu. Elini uzattÄ±. â€” Bu bÃ¼yÃ¼k gÃ¼nden bana ne kadar adam olduÄŸumu gÃ¶rmem. Ä°ÅŸte ben de her sabah bir ÅŸey olacak. â€” Ne vakit?.. â€” Mesela... Niyazi misalini sÃ¶yleyemez. Sokaktan ÅŸiddetli, keskin, gÃ¼r, canlÄ±, latif, parlak, ahenkli bir ses gelir. \"DÃ¼nya deÄŸiÅŸti. Eski gÃ¼nler geÃ§ti. Merhamet, mÃ¼rÃ¼vvet, insaniyet kalmadÄ±. Herkes keyfinde, eÄŸlencesinde, kimse kimseyi dÃ¼ÅŸÃ¼nmez oldu. Bu ne haldir?\" Birbirlerinin yÃ¼zlerine bakÄ±ÅŸÄ±rlar. Gayri ihtiyarÃ®, sÃ¢rÃ® bir hareketle, tellenmiÅŸ cigaralarÄ±nÄ± Ã¶nlerindeki tablada sÃ¶ndÃ¼rÃ¼rler: â€” Bu ne? â€” Bilmem. Sokaktan gelen ahenkli ses aynÄ± ÅŸiddet, aynÄ± letafet, aynÄ± azamet, aynÄ± belÃ¢gat le devam eder: \"...DÃ¼nya bir cife dir. Hayf onu isteyen kÃ¶peklere! UyanÄ±n, kÃ¢inata ibretle bakÄ±n. FÃ¢ni olan ÅŸeylere aldanmayÄ±n.\" NeÅŸet bir eliyle baÅŸÄ±nÄ±, diÄŸer eliyle sarÄ± pijamasÄ±nÄ±n Ã¼stÃ¼nden kalÃ§asÄ±nÄ± kaÅŸÄ±r. YÃ¼zÃ¼nÃ¼ ekÅŸitir. Niyazi alt dudaÄŸÄ±nÄ± Ä±sÄ±rarak yavaÅŸ yavaÅŸ doÄŸrulur: â€” Bu ne be? â€” SaÃ§ma... â€” Ama ne gÃ¼zel, ne yanÄ±k sÃ¶ylÃ¼yor... â€” Gayet muktedir bir hatip olacak. â€” ÅÃ¼phesiz. Sokaktan gelen ses: \"...BugÃ¼n varÄ±z, yarÄ±n yok! GÃ¼ndÃ¼zÃ¼n sonu gece. AydÄ±nlÄ±ÄŸÄ±n sonu karanlÄ±k. AteÅŸin sonu kÃ¼l. HayatÄ±n sonu Ã¶lÃ¼m... Ã–lÃ¼mden kim ÅŸÃ¼phe eder? AltÄ±nlara gark olsak , demirden, Ã§elikten kaleler iÃ§ine saklansak, mutlaka Ã¶lÃ¼m oku gelip bizi bulacak. Er geÃ§ bize yetiÅŸecek. Bu kadar muhakkak bir akibet karÅŸÄ±sÄ±nda gaflete dÃ¼ÅŸen, nefsine uyan, yarÄ±nÄ± unutan insan mÄ±dÄ±r? HayÄ±r... HayvandÄ±r. Nefsine uyanlarÄ±n, zevkten baÅŸka bir ÅŸey tanÄ±mayanlarÄ±n, hayvanlardan ne farkÄ± var?\" NeÅŸet ayaÄŸa kalkar, der ki: â€” Bu bir feylesof! â€” Hem de derin... â€” Hem pek derin bir feylesof! â€” Bak, bak, ne diyor? Sokaktan gelen ses daha yanÄ±k bir hararet, daha azametli bir ÅŸiddetle: \"...Merhamet, ÅŸefkat, elÃ¢lem, kimsenin umurunda deÄŸil. SadakanÄ±n ismi unutulmuÅŸ. Yiyiniz, iÃ§iniz, keyif ediniz... Ã‡alÄ±nÄ±z. OynayÄ±nÄ±z. GÃ¼zel evlerin iÃ§inde, temiz karyolalarda, rahat rahat gÃ¼ndÃ¼z uykularÄ±na yatÄ±nÄ±z. Ah, nerede fazilet?\" Niyazi oturduÄŸu yerden: â€” Hem sosyalist, be! der. â€” Ne demek?.. â€” GÃ¼pe\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f5efba",
   "metadata": {},
   "source": [
    "# Comments\n",
    "Results show that underlying grammar patterns can be easily extracted (huge decrease in the loss value). After that, according to the experiments, it learns harder patterns to decrease the loss further because easier patterns are already learned (grammar patterns). Those harder patterns include the semantic information of the texts. I experimented with two different layers (i.e. I experimented 1 layer and 2 layer LSTM model.).\n",
    "\n",
    "# Results\n",
    "It is clear to see that 1-layer LSTM copied the text mostly, whereas 2-layer LSTM preserves the style of the author while do not overfitting it. It is due to more complex representation of the 2-layer LSTM, which makes it harder to overfit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pureTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
